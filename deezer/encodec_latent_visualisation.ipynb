{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8a683-ee44-46df-a6f4-d5c35f773525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sure to have torch installed and numpy<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b699c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run only if on colab\n",
    "# to download a file from google drive, share it and get the id from the share link (be sure that you can share it with the link)\n",
    "!pip install encodec\n",
    "!pip install numpy<2\n",
    "!pip install torchcodec\n",
    "!gdown 1Rj84epzYkfj00-B79nRdU4h3fquwUBsS\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1EM4F6Rp2nf7E79toGGHQ68qgprYFa9VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a4188-e10b-4704-a6c1-545c740da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec import EncodecModel   # `pip install encodec` if unavailable\n",
    "\n",
    "model = EncodecModel.encodec_model_48khz()  # will download and cache the weights when run for the first time\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907242-9a55-4ca3-9d69-ef084d477c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "stft_transformer = torchaudio.transforms.Spectrogram(n_fft=1<<13, power=2)\n",
    "stft_transformer_device = torchaudio.transforms.Spectrogram(n_fft=1<<13, power=2).to(device)\n",
    "def compute_fp_stft(p):\n",
    "    stft = stft_transformer(torch.Tensor(p)).numpy()\n",
    "    stft = 10 * np.log10(np.clip( stft, 1e-10, 1e6 ) )\n",
    "    fingerprint = np.mean(stft, axis=(2) ) \n",
    "    return fingerprint\n",
    "\n",
    "def compute_stft_full_cpu(p):\n",
    "        stft = stft_transformer(torch.Tensor(p))\n",
    "        stft = 10 * torch.log10(torch.clamp(stft, 1e-10, 1e6))\n",
    "        return stft.numpy()\n",
    "\n",
    "def compute_stft_full_device(p):\n",
    "        stft = stft_transformer_device(p)\n",
    "        stft = 10 * torch.log10(torch.clamp(stft, 1e-10, 1e6))\n",
    "        return stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70240beb-1821-4061-b8f3-d421efda236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"Fair-L - War Song - Blood and Rose.mp3\"\n",
    "audio, sr = torchaudio.load(audio_path, channels_first = False)\n",
    "audio = torch.Tensor(audio.T).unsqueeze(0).to(device)\n",
    "frames = model.encode(audio)\n",
    "\n",
    "fft_audio_input = compute_stft_full_device(audio[0])\n",
    "\n",
    "# we ignore the output scaling and linear interpolation between frames (1 sec audio) and directly decode everything\n",
    "codes = torch.concatenate([a[0] for a in frames], -1)\n",
    "with torch.no_grad():\n",
    "    emb = model.quantizer.decode(codes.transpose(0, 1))\n",
    "    post_lstm = model.decoder.model[:2](emb)\n",
    "\n",
    "audio_latent = post_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a8d26-a2d6-4410-a00f-21233e9de9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detects GPU/CPU => run on GPU or CPU, time saving : from 2min to 5s\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "layers = [\n",
    "    {\"l\": 4, \"stride\": 8, \"title\": \"(3): ConvTranspose(stride=8)\"},\n",
    "    {\"l\": 7, \"stride\": 5, \"title\": \"(6): ConvTranspose(stride=5)\"},\n",
    "    {\"l\": 10, \"stride\": 4, \"title\": \"(9): ConvTranspose(stride=4)\"},\n",
    "    {\"l\": 13, \"stride\": 2, \"title\": \"(12): ConvTranspose(stride=2)\"},\n",
    "]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "    audio_latent_device = audio_latent.to(device)\n",
    "    \n",
    "    input_stft = compute_stft_full_device(audio_latent_device[0]).cpu().numpy()\n",
    "    mean_input_fft = np.mean(input_stft, axis=2)\n",
    "\n",
    "    ffts = []\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for layer in layers:\n",
    "            latent = model.decoder.model[2:layer[\"l\"]](audio_latent_device)\n",
    "            print(f\"l: {layer['l']}, latent.shape: {latent.shape}\")\n",
    "            latents.append(latent)\n",
    "            stft_latent = compute_stft_full_device(latent[0]).cpu().numpy()\n",
    "            ffts.append(np.mean(stft_latent, axis=2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_rec = model.decoder.model[2:](audio_latent_device)\n",
    "    output_stft = compute_stft_full_device(audio_rec[0]).cpu().numpy()\n",
    "    output_fft = np.mean(output_stft, axis=2)\n",
    "\n",
    "    print(\"GPU processing complete!\")\n",
    "else:\n",
    "    input_stft = compute_stft_full_cpu(audio_latent[0])\n",
    "    mean_input_fft = np.mean(input_stft, axis=2)\n",
    "\n",
    "    ffts = []\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for layer in layers:\n",
    "            latent = model.decoder.model[2:layer[\"l\"]](audio_latent)\n",
    "            latents.append(latent)\n",
    "            stft_latent = compute_stft_full_cpu(latent[0])\n",
    "            ffts.append(np.mean(stft_latent, axis=2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_rec = model.decoder.model[2:](audio_latent)\n",
    "    output_stft = compute_stft_full_cpu(audio_rec[0])\n",
    "    output_fft = np.mean(output_stft, axis=2)\n",
    "\n",
    "    print(\"CPU processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2d010-3c42-45cc-83f5-0dfdeae578e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fractal(k=8, N=4097, y_offset=0):\n",
    "    \"\"\" Display where the peaks are supposed to be. \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for t in range(0, k//2+1):\n",
    "        x.append(t*2*N/k)\n",
    "        y.append(y_offset)\n",
    "    plt.scatter(x, y, c=\"magenta\", marker=\"x\", linewidth=2, clip_on=False)\n",
    "\n",
    "plt.figure(figsize=(8, 20))\n",
    "plt.subplot(7, 1, 1)\n",
    "fft_audio_input_mean = np.mean(fft_audio_input.cpu().numpy(), axis=2)\n",
    "plt.plot(np.mean(fft_audio_input_mean, 0), color=\"red\")\n",
    "plt.title(\"input signal spectrum\", fontsize=10)\n",
    "plt.xticks([0, 2048, 4096], [\"0Hz\", \"12kHz\", \"24kHz\"], fontsize=8)\n",
    "plt.yticks([], [])\n",
    "plt.ylabel(\"amplitude (dB)\", fontsize=8)\n",
    "\n",
    "plt.subplot(7, 1, 2)\n",
    "plt.plot(np.mean(mean_input_fft, 0), color=\"red\")\n",
    "plt.title(\"encoded signal spectrum\", fontsize=10)\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "    plt.subplot(7, 1, 3 + i)\n",
    "    base = np.mean(ffts[i], 0)\n",
    "    plt.plot(base, label=\"original\")\n",
    "    plt.title(layer[\"title\"], fontsize=10)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plot_fractal(k=layer[\"stride\"], y_offset=np.mean(base)-8)\n",
    "\n",
    "plt.subplot(7, 1, 7)\n",
    "plt.plot(np.mean(output_fft, 0), label=\"output\", color=\"red\")\n",
    "plt.title(\"generated signal spectrum\", fontsize=10)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks([], [])\n",
    "plt.xticks([0, 2048, 4096], [\"0Hz\", \"12kHz\", \"24kHz\"], fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98b36e",
   "metadata": {},
   "source": [
    "## Audio Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch_spec(spec, rate):\n",
    "    c, f, t = spec.shape\n",
    "    new_t = int(np.round(t / rate))\n",
    "    idx = np.linspace(0, t - 1, new_t)\n",
    "    out = np.empty((c, f, new_t), dtype=spec.dtype)\n",
    "    for ci in range(c):\n",
    "        for fi in range(f):\n",
    "            out[ci, fi] = np.interp(idx, np.arange(t), spec[ci, fi])\n",
    "    return out\n",
    "\n",
    "def resample_time_tensor(x, rate):\n",
    "    # x: [C, T], resample along time to change pitch\n",
    "    c, t = x.shape\n",
    "    new_t = int(np.round(t / rate))\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x_t = x.unsqueeze(0)  # [1, C, T]\n",
    "        x_rs = torch.nn.functional.interpolate(x_t, size=new_t, mode=\"linear\", align_corners=False)\n",
    "        return x_rs.squeeze(0)\n",
    "    idx = np.linspace(0, t - 1, new_t)\n",
    "    out = np.empty((c, new_t), dtype=x.dtype)\n",
    "    for ci in range(c):\n",
    "        out[ci] = np.interp(idx, np.arange(t), x[ci])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05715fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME-STRETCH comparison\n",
    "stretch_rate = 1.2\n",
    "\n",
    "# Simple algo: linear interpolation in time domain on latent\n",
    "latent0 = latents[0][0] if torch.cuda.is_available() else latents[0][0]\n",
    "latent0_simple_ts = resample_time_tensor(latent0, rate=stretch_rate)\n",
    "\n",
    "# Real algo: phase vocoder on latent's spectrogram (STFT time-warp + Griffin-Lim)\n",
    "n_fft_latent = 1024\n",
    "hop_length_latent = 256\n",
    "win_length_latent = n_fft_latent\n",
    "window_latent = torch.hann_window(win_length_latent)\n",
    "\n",
    "latent0_cpu = latent0.detach().cpu()\n",
    "complex_spec_latent = torch.stft(latent0_cpu, n_fft=n_fft_latent, hop_length=hop_length_latent, \n",
    "                                  win_length=win_length_latent, window=window_latent, return_complex=True)\n",
    "mag_latent = complex_spec_latent.abs().numpy()\n",
    "mag_latent_ts = time_stretch_spec(mag_latent, rate=stretch_rate)\n",
    "mag_latent_ts_torch = torch.from_numpy(mag_latent_ts)\n",
    "\n",
    "latent0_real_ts = []\n",
    "for c in range(mag_latent_ts_torch.shape[0]):\n",
    "    recon = torchaudio.functional.griffinlim(\n",
    "        mag_latent_ts_torch[c],\n",
    "        window=window_latent,\n",
    "        n_fft=n_fft_latent,\n",
    "        hop_length=hop_length_latent,\n",
    "        win_length=win_length_latent,\n",
    "        power=1.0,\n",
    "        n_iter=32,\n",
    "        momentum=0.99,\n",
    "        length=int(latent0_cpu.shape[-1] / stretch_rate),\n",
    "        rand_init=True\n",
    "    )\n",
    "    latent0_real_ts.append(recon)\n",
    "latent0_real_ts = torch.stack(latent0_real_ts, dim=0)\n",
    "\n",
    "# Compute FFTs for visualization\n",
    "if torch.cuda.is_available():\n",
    "    stft0_simple = compute_stft_full_device(latent0_simple_ts.to(device)).cpu().numpy()\n",
    "    stft0_real = compute_stft_full_device(latent0_real_ts.to(device)).cpu().numpy()\n",
    "else:\n",
    "    stft0_simple = compute_stft_full_cpu(latent0_simple_ts)\n",
    "    stft0_real = compute_stft_full_cpu(latent0_real_ts)\n",
    "\n",
    "fft0_simple_ts = np.mean(stft0_simple, axis=2)\n",
    "fft0_real_ts = np.mean(stft0_real, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366885b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize peak shifting: Original vs Simple vs Real algorithm\n",
    "base = np.mean(ffts[0], 0)\n",
    "fft_simple_mean = np.mean(fft0_simple_ts, 0)\n",
    "fft_real_mean = np.mean(fft0_real_ts, 0)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base, label=\"Original latent\", color=\"#1f77b4\", linewidth=2.5, alpha=0.9)\n",
    "plt.plot(fft_simple_mean, label=f\"Simple (linear interp) x{stretch_rate}\", \n",
    "         color=\"#ff7f0e\", linestyle=\":\", linewidth=2.0, alpha=0.8)\n",
    "plt.plot(fft_real_mean, label=f\"Real (STFT+Griffin-Lim) x{stretch_rate}\", \n",
    "         color=\"#2ca02c\", linestyle=\"--\", linewidth=2.0, alpha=0.8)\n",
    "plt.title(\"Time-Stretch: Peak Shifting in Frequency Domain\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Frequency Bins\", fontsize=10)\n",
    "plt.ylabel(\"Amplitude (dB)\", fontsize=10)\n",
    "plt.xticks([0, 2048, 4096], [\"0Hz\", \"12kHz\", \"24kHz\"], fontsize=9)\n",
    "plt.legend(fontsize=9, loc='upper right')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plot_fractal(k=layers[0][\"stride\"], y_offset=np.mean(base)-8)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base, label=\"Original\", color=\"#1f77b4\", linewidth=2.5, alpha=0.9)\n",
    "plt.plot(fft_simple_mean, label=\"Simple algorithm\", color=\"#ff7f0e\", linestyle=\":\", linewidth=2.0, alpha=0.8)\n",
    "plt.plot(fft_real_mean, label=\"Real algorithm\", color=\"#2ca02c\", linestyle=\"--\", linewidth=2.0, alpha=0.8)\n",
    "plt.title(\"Zoomed View (0-2kHz)\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Frequency Bins\", fontsize=10)\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(np.min(base[:1000])-5, np.max(base[:1000])+5)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"time_stretch_peak_shift_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to time-stretch on REAL AUDIO\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# First decode the original audio from audio_latent (post_lstm)\n",
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        audio_original = model.decoder.model[2:](audio_latent.to(device)).cpu()\n",
    "    else:\n",
    "        audio_original = model.decoder.model[2:](audio_latent)\n",
    "\n",
    "wave_original = audio_original.squeeze(0)\n",
    "\n",
    "# Apply time-stretch SIMPLE: linear interpolation on decoded audio\n",
    "wave_simple_ts = resample_time_tensor(wave_original, rate=stretch_rate)\n",
    "\n",
    "# Apply time-stretch REAL: STFT + Griffin-Lim on decoded audio\n",
    "n_fft_audio = 2048\n",
    "hop_length_audio = 512\n",
    "win_length_audio = n_fft_audio\n",
    "window_audio = torch.hann_window(win_length_audio)\n",
    "\n",
    "wave_cpu = wave_original.cpu()\n",
    "complex_spec_audio = torch.stft(wave_cpu, n_fft=n_fft_audio, hop_length=hop_length_audio,\n",
    "                                 win_length=win_length_audio, window=window_audio, return_complex=True)\n",
    "mag_audio = complex_spec_audio.abs().numpy()\n",
    "mag_audio_ts = time_stretch_spec(mag_audio, rate=stretch_rate)\n",
    "mag_audio_ts_torch = torch.from_numpy(mag_audio_ts)\n",
    "\n",
    "wave_real_ts = []\n",
    "for c in range(mag_audio_ts_torch.shape[0]):\n",
    "    recon = torchaudio.functional.griffinlim(\n",
    "        mag_audio_ts_torch[c],\n",
    "        window=window_audio,\n",
    "        n_fft=n_fft_audio,\n",
    "        hop_length=hop_length_audio,\n",
    "        win_length=win_length_audio,\n",
    "        power=1.0,\n",
    "        n_iter=32,\n",
    "        momentum=0.99,\n",
    "        length=int(wave_cpu.shape[-1] / stretch_rate),\n",
    "        rand_init=True\n",
    "    )\n",
    "    wave_real_ts.append(recon)\n",
    "wave_real_ts = torch.stack(wave_real_ts, dim=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO COMPARISON: Original vs Time-Stretched (Simple vs Real)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n\ud83c\udfb5 Original Audio\")\n",
    "display(Audio(wave_original.numpy(), rate=sr))\n",
    "\n",
    "print(f\"\\n\ud83c\udfb5 Time-Stretch x{stretch_rate} - SIMPLE (linear interpolation)\")\n",
    "display(Audio(wave_simple_ts.numpy(), rate=sr))\n",
    "\n",
    "print(f\"\\n\ud83c\udfb5 Time-Stretch x{stretch_rate} - REAL (STFT + Griffin-Lim)\")\n",
    "display(Audio(wave_real_ts.numpy(), rate=sr))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Notice: Real algorithm preserves frequency content better\")\n",
    "print(\"Simple algorithm may introduce artifacts and frequency distortion\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1880210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PITCH-SHIFT comparison\n",
    "pitch_rate = 0.8\n",
    "\n",
    "# Simple algo: linear interpolation (time-domain resampling changes pitch)\n",
    "latent0_simple_ps = resample_time_tensor(latent0, rate=pitch_rate)\n",
    "\n",
    "# Real algo: resampling with anti-aliasing filter\n",
    "latent0_real_ps = torchaudio.functional.resample(\n",
    "    latent0.cpu(), \n",
    "    orig_freq=int(1000),  \n",
    "    new_freq=int(1000 * pitch_rate)\n",
    ")\n",
    "\n",
    "# Compute FFTs for visualization\n",
    "if torch.cuda.is_available():\n",
    "    stft0_simple_ps = compute_stft_full_device(latent0_simple_ps.to(device)).cpu().numpy()\n",
    "    stft0_real_ps = compute_stft_full_device(latent0_real_ps.to(device)).cpu().numpy()\n",
    "else:\n",
    "    stft0_simple_ps = compute_stft_full_cpu(latent0_simple_ps)\n",
    "    stft0_real_ps = compute_stft_full_cpu(latent0_real_ps)\n",
    "\n",
    "fft0_simple_ps = np.mean(stft0_simple_ps, axis=2)\n",
    "fft0_real_ps = np.mean(stft0_real_ps, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize peak shifting for pitch-shift: Original vs Simple vs Real\n",
    "base = np.mean(ffts[0], 0)\n",
    "fft_simple_ps_mean = np.mean(fft0_simple_ps, 0)\n",
    "fft_real_ps_mean = np.mean(fft0_real_ps, 0)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base, label=\"Original latent\", color=\"#1f77b4\", linewidth=2.5, alpha=0.9)\n",
    "plt.plot(fft_simple_ps_mean, label=f\"Simple (linear interp) x{pitch_rate}\", \n",
    "         color=\"#d62728\", linestyle=\":\", linewidth=2.0, alpha=0.8)\n",
    "plt.plot(fft_real_ps_mean, label=f\"Real (resample w/ anti-alias) x{pitch_rate}\", \n",
    "         color=\"#9467bd\", linestyle=\"--\", linewidth=2.0, alpha=0.8)\n",
    "plt.title(\"Pitch-Shift: Peak Shifting in Frequency Domain\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Frequency Bins\", fontsize=10)\n",
    "plt.ylabel(\"Amplitude (dB)\", fontsize=10)\n",
    "plt.xticks([0, 2048, 4096], [\"0Hz\", \"12kHz\", \"24kHz\"], fontsize=9)\n",
    "plt.legend(fontsize=9, loc='upper right')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plot_fractal(k=layers[0][\"stride\"], y_offset=np.mean(base)-8)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base, label=\"Original\", color=\"#1f77b4\", linewidth=2.5, alpha=0.9)\n",
    "plt.plot(fft_simple_ps_mean, label=\"Simple algorithm\", color=\"#d62728\", linestyle=\":\", linewidth=2.0, alpha=0.8)\n",
    "plt.plot(fft_real_ps_mean, label=\"Real algorithm\", color=\"#9467bd\", linestyle=\"--\", linewidth=2.0, alpha=0.8)\n",
    "plt.title(\"Zoomed View (0-2kHz) - Note Frequency Shift\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Frequency Bins\", fontsize=10)\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(np.min(base[:1000])-5, np.max(base[:1000])+5)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pitch_shift_peak_shift_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to pitch-shift on REAL AUDIO\n",
    "\n",
    "# Apply pitch-shift SIMPLE: linear interpolation on decoded audio\n",
    "wave_simple_ps = resample_time_tensor(wave_original, rate=pitch_rate)\n",
    "\n",
    "# Apply pitch-shift REAL: resample with anti-aliasing on decoded audio\n",
    "wave_real_ps = torchaudio.functional.resample(\n",
    "    wave_original,\n",
    "    orig_freq=int(sr),\n",
    "    new_freq=int(sr * pitch_rate)\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO COMPARISON: Original vs Pitch-Shifted (Simple vs Real)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n\ud83c\udfb5 Original Audio\")\n",
    "display(Audio(wave_original.numpy(), rate=sr))\n",
    "\n",
    "print(f\"\\n\ud83c\udfb5 Pitch-Shift x{pitch_rate} - SIMPLE (linear interpolation)\")\n",
    "display(Audio(wave_simple_ps.numpy(), rate=sr))\n",
    "\n",
    "print(f\"\\n\ud83c\udfb5 Pitch-Shift x{pitch_rate} - REAL (resample with anti-aliasing)\")\n",
    "display(Audio(wave_real_ps.numpy(), rate=sr))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Notice: Pitch shift changes frequency content\")\n",
    "print(\"Real algorithm uses anti-aliasing to prevent aliasing artifacts\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}